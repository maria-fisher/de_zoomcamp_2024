{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5db21097-3dd7-4f79-9f73-9574927ace30",
   "metadata": {},
   "source": [
    "# Data Engineering ZoomCamp\n",
    "## Week 5: Batch Processing\n",
    "Maria Fisher \n",
    "\n",
    "In this homework we'll put what we learned about Spark in practice.\n",
    "\n",
    "For this homework we will be using the FHV 2019-10 data found here. [FHV Data](https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-10.csv.gz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c6146a77-269d-426d-ac9c-16bacb6ed73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d364e9-e71e-4f08-b207-9a6c451aace4",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Question 1: \n",
    "\n",
    "**Install Spark and PySpark** \n",
    "\n",
    "- Install Spark\n",
    "- Run PySpark\n",
    "- Create a local spark session\n",
    "- Execute spark.version.\n",
    "\n",
    "What's the output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b2ab30db-268f-4353-99b7-c449f33292b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.3.2'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d65d082-686f-4bf1-a2b1-ed7cb085206f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pyspark.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3013e0-18a6-4e30-b496-ff9372fd04c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://github.com/DataTalksClub/nyc-tlc-data/releases/download/fhv/fhv_tripdata_2019-10.csv.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b216fcfb-6b3c-4ce8-812c-c758de5ae0f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/01 15:51:12 WARN Utils: Your hostname, river resolves to a loopback address: 127.0.1.1; using 192.168.1.252 instead (on interface wlp1s0)\n",
      "24/03/01 15:51:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/01 15:51:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/03/01 15:51:14 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "24/03/01 15:51:14 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName('test') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "109ad7fe-7ac6-494b-8337-c17d6b5317db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r-- 1 malu malu 115M Nov 21  2022 fhv_tripdata_2019-10.csv\n"
     ]
    }
   ],
   "source": [
    "!ls -lh fhv_tripdata_2019-10.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9e3bc82-47e6-4202-bfb8-fc7042f0ad93",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1897494 fhv_tripdata_2019-10.csv\n"
     ]
    }
   ],
   "source": [
    "!wc -l fhv_tripdata_2019-10.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6bfe139-c789-48e5-8976-5d8a63d59d31",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('fhv_tripdata_2019-10.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79bf2ef0-8b73-476e-8297-888e04b98ea6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('dispatching_base_num', StringType(), True), StructField('pickup_datetime', StringType(), True), StructField('dropOff_datetime', StringType(), True), StructField('PUlocationID', StringType(), True), StructField('DOlocationID', StringType(), True), StructField('SR_Flag', StringType(), True), StructField('Affiliated_base_number', StringType(), True)])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227df6da-b85b-49b4-bb4c-93e5e515b209",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fed249c4-fcde-49b9-a0af-3f05c508500e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dispatching_base_num', 'string'),\n",
       " ('pickup_datetime', 'timestamp'),\n",
       " ('dropOff_datetime', 'timestamp'),\n",
       " ('PULocationID', 'int'),\n",
       " ('DOLocationID', 'int'),\n",
       " ('SR_Flag', 'string'),\n",
       " ('Affiliated_base_number', 'string')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e9adb7-17df-4949-a2fd-e8bb80cf091f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0af134b1-b8d3-44ac-9b84-7a7bfff6efee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b421080-779d-4830-b0c4-62f754de843c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "schema = types.StructType([\n",
    "    types.StructField('dispatching_base_num', types.StringType(), True),\n",
    "    types.StructField('pickup_datetime', types.TimestampType(), True),\n",
    "    types.StructField('dropOff_datetime', types.TimestampType(), True),\n",
    "    types.StructField('PULocationID', types.IntegerType(), True),\n",
    "    types.StructField('DOLocationID', types.IntegerType(), True),\n",
    "    types.StructField('SR_Flag', types.StringType(), True),\n",
    "    types.StructField('Affiliated_base_number', types.StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab577131-c124-4e2c-8474-ed5f0ee37473",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(dispatching_base_num='B00009', pickup_datetime='2019-10-01 00:23:00', dropOff_datetime='2019-10-01 00:35:00', PUlocationID='264', DOlocationID='264', SR_Flag=None, Affiliated_base_number='B00009')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e51e69-6d58-4e40-a044-5be4bb036d0e",
   "metadata": {},
   "source": [
    "### Question 2: \n",
    "\n",
    "**FHV October 2019**\n",
    "\n",
    "Read the October 2019 FHV into a Spark Dataframe with a schema as we did in the lessons.\n",
    "\n",
    "Repartition the Dataframe to 6 partitions and save it to parquet.\n",
    "\n",
    "What is the average size of the Parquet (ending with .parquet extension) Files that were created (in MB)? Select the answer which most closely matches.\n",
    "\n",
    "- 1MB\n",
    "- 6MB x\n",
    "- 25MB\n",
    "- 87MB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66710f69-7c50-4d05-bf50-df3406df5af6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv('fhv_tripdata_2019-10.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9f86199b-5246-4c16-9612-c2852e3b9b04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.repartition(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2ffd132f-cce4-4a75-9142-5d46bbbfc58d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.parquet('fhv/2019/10/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "726146ce-78b9-4b8f-8ddf-0e122bb295bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = spark.read.parquet('fhv/2019/10/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b955b50a-e419-4e1b-9bff-a3b3acb193c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dispatching_base_num: string (nullable = true)\n",
      " |-- pickup_datetime: timestamp (nullable = true)\n",
      " |-- dropOff_datetime: timestamp (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- SR_Flag: string (nullable = true)\n",
      " |-- Affiliated_base_number: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8ec134-50a8-48a8-b152-32d7181c88c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c06f81f6-5808-4cfe-b4f8-36993f50a7ba",
   "metadata": {},
   "source": [
    "### Question 3: \n",
    "\n",
    "**Count records** \n",
    "\n",
    "How many taxi trips were there on the 15th of October?\n",
    "\n",
    "Consider only trips that started on the 15th of October.\n",
    "\n",
    "- 108,164\n",
    "- 12,856\n",
    "- 452,470\n",
    "- 62,610 x\n",
    "\n",
    "> [!IMPORTANT]\n",
    "> Be aware of columns order when defining schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b12db26-73af-4bf0-8baa-ca0afae8e405",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7abae1ad-aaaf-444b-98aa-9029249cae13",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|dispatching_base_num|    pickup_datetime|   dropOff_datetime|PULocationID|DOLocationID|SR_Flag|Affiliated_base_number|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "|              B00445|2019-10-07 03:54:41|2019-10-07 04:02:41|         252|         138|   null|                B00445|\n",
      "|              B02613|2019-10-05 00:47:05|2019-10-05 01:11:53|         264|          39|   null|                B02613|\n",
      "|              B01437|2019-10-02 20:29:31|2019-10-02 20:52:31|         264|         173|   null|                B01437|\n",
      "|              B02292|2019-10-02 16:40:52|2019-10-02 16:53:57|         264|          17|   null|                B02292|\n",
      "|              B01338|2019-10-03 17:21:08|2019-10-03 17:26:58|         264|          32|   null|                B01338|\n",
      "|              B02715|2019-10-08 13:10:21|2019-10-08 13:27:26|         236|         233|   null|                B02788|\n",
      "|              B03060|2019-10-04 17:23:21|2019-10-04 17:46:10|         264|          22|   null|                B03060|\n",
      "|              B02285|2019-10-02 19:55:00|2019-10-02 21:07:00|         264|         264|   null|                B02285|\n",
      "|              B02311|2019-10-06 02:03:52|2019-10-06 02:17:22|         264|          89|   null|                B02311|\n",
      "|              B01315|2019-10-08 13:18:59|2019-10-08 13:35:23|         264|          78|   null|                B01315|\n",
      "|              B02285|2019-10-01 20:08:11|2019-10-01 21:01:00|         264|         264|   null|                B02285|\n",
      "|              B02293|2019-10-02 10:25:20|2019-10-02 12:49:37|          74|          61|   null|                B02293|\n",
      "|              B01087|2019-10-01 08:52:12|2019-10-01 16:00:14|          43|          43|   null|                B01087|\n",
      "|              B03160|2019-10-05 06:19:00|2019-10-05 06:31:00|          60|         184|   null|                B02133|\n",
      "|              B01087|2019-10-03 22:14:32|2019-10-03 22:43:34|         142|         235|   null|                B01087|\n",
      "|              B00256|2019-10-02 21:25:36|2019-10-02 21:58:51|         264|         264|   null|                B00256|\n",
      "|              B00900|2019-10-03 11:57:10|2019-10-03 12:02:54|         264|         197|   null|                B00900|\n",
      "|              B02905|2019-10-02 09:24:09|2019-10-02 09:30:10|         264|         228|   null|                B02905|\n",
      "|              B00821|2019-10-07 04:01:35|2019-10-07 04:07:42|         264|          77|   null|                B00821|\n",
      "|              B02838|2019-10-05 19:00:36|2019-10-05 19:10:26|         264|         133|   null|                B02838|\n",
      "+--------------------+-------------------+-------------------+------------+------------+-------+----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12c25f68-459a-4fb4-9720-4ff44f2f03dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "62610"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df \\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .filter(\"pickup_date = '2019-10-15'\") \\\n",
    "    .count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7effae3-f3fc-4fc8-8e58-69a2838cf6f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b16e6e3-79bd-4068-a9af-58b66f5922a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9401535-4991-456f-b29e-42b4b5a63386",
   "metadata": {},
   "source": [
    "### Question 4: \n",
    "\n",
    "**Longest trip for each day** \n",
    "\n",
    "What is the length of the longest trip in the dataset in hours?\n",
    "\n",
    "- 631,152.50 Hours x\n",
    "- 243.44 Hours\n",
    "- 7.68 Hours\n",
    "- 3.32 Hours\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5bc0c3a3-3d0d-4362-b810-68cb121e0d57",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:================================================>         (5 + 1) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------------+\n",
      "|pickup_date|max(duration_hours)|\n",
      "+-----------+-------------------+\n",
      "| 2019-10-28|           631152.5|\n",
      "| 2019-10-11|           631152.5|\n",
      "| 2019-10-31|  87672.44083333333|\n",
      "| 2019-10-01|  70128.02805555555|\n",
      "| 2019-10-17|             8794.0|\n",
      "+-----------+-------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df \\\n",
    "    .withColumn('duration_seconds', F.unix_timestamp(df.dropOff_datetime) - F.unix_timestamp(df.pickup_datetime)) \\\n",
    "    .withColumn('duration_hours', F.col('duration_seconds') / 3600) \\\n",
    "    .withColumn('pickup_date', F.to_date(df.pickup_datetime)) \\\n",
    "    .groupBy('pickup_date') \\\n",
    "    .max('duration_hours') \\\n",
    "    .orderBy('max(duration_hours)', ascending=False) \\\n",
    "    .limit(5) \\\n",
    "    .show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4802842f-9d09-49fb-9029-47c606efd000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4f5c3bed-20bf-4920-810b-fd68149d59c8",
   "metadata": {},
   "source": [
    "### Question 5: \n",
    "\n",
    "**User Interface**\n",
    "\n",
    "Sparkâ€™s User Interface which shows the application's dashboard runs on which local port?\n",
    "\n",
    "- 80\n",
    "- 443\n",
    "- 4040 x\n",
    "- 8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72a46c6-2872-4c57-9eb4-6b3cf01b3cfc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "502d6404-81ca-4c57-8f21-1fbb5a0e03a9",
   "metadata": {},
   "source": [
    "### Question 6: \n",
    "\n",
    "**Least frequent pickup location zone**\n",
    "\n",
    "Load the zone lookup data into a temp view in Spark</br>\n",
    "[Zone Data](https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv)\n",
    "\n",
    "Using the zone lookup data and the FHV October 2019 data, what is the name of the LEAST frequent pickup location Zone?</br>\n",
    "\n",
    "- East Chelsea\n",
    "- Jamaica Bay x\n",
    "- Union Sq\n",
    "- Crown Heights North"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "764e21c7-40cd-4f0e-b77c-c57cb2e33a99",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+--------------------+------------+\n",
      "|LocationID|      Borough|                Zone|service_zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "|         1|          EWR|      Newark Airport|         EWR|\n",
      "|         2|       Queens|         Jamaica Bay|   Boro Zone|\n",
      "|         3|        Bronx|Allerton/Pelham G...|   Boro Zone|\n",
      "|         4|    Manhattan|       Alphabet City| Yellow Zone|\n",
      "|         5|Staten Island|       Arden Heights|   Boro Zone|\n",
      "|         6|Staten Island|Arrochar/Fort Wad...|   Boro Zone|\n",
      "|         7|       Queens|             Astoria|   Boro Zone|\n",
      "|         8|       Queens|        Astoria Park|   Boro Zone|\n",
      "|         9|       Queens|          Auburndale|   Boro Zone|\n",
      "|        10|       Queens|        Baisley Park|   Boro Zone|\n",
      "|        11|     Brooklyn|          Bath Beach|   Boro Zone|\n",
      "|        12|    Manhattan|        Battery Park| Yellow Zone|\n",
      "|        13|    Manhattan|   Battery Park City| Yellow Zone|\n",
      "|        14|     Brooklyn|           Bay Ridge|   Boro Zone|\n",
      "|        15|       Queens|Bay Terrace/Fort ...|   Boro Zone|\n",
      "|        16|       Queens|             Bayside|   Boro Zone|\n",
      "|        17|     Brooklyn|             Bedford|   Boro Zone|\n",
      "|        18|        Bronx|        Bedford Park|   Boro Zone|\n",
      "|        19|       Queens|           Bellerose|   Boro Zone|\n",
      "|        20|        Bronx|             Belmont|   Boro Zone|\n",
      "+----------+-------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_zones = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv('taxi+_zone_lookup.csv')\n",
    "\n",
    "df_zones.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8d39bc58-6dca-4fd5-94f2-6a5b4b09e923",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Join Fhv with zone lookup data\n",
    "joined_data = df.join(df_zones, df['PULocationID'] == df_zones['LocationID'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0b5c1a77-0087-4906-b939-331af65c0ff1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[dispatching_base_num: string, pickup_datetime: timestamp, dropOff_datetime: timestamp, PULocationID: int, DOLocationID: int, SR_Flag: string, Affiliated_base_number: string, LocationID: string, Borough: string, Zone: string, service_zone: string]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a5f26307-a03a-4cf8-9c6e-978c2f245133",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Group by pickup zone ans count\n",
    "df_zones = joined_data.groupBy('Zone').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bd73ea21-5e5c-4ec0-94b5-097ae932dac4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:==========================================================(6 + 0) / 6]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|                Zone|count|\n",
      "+--------------------+-----+\n",
      "|         Jamaica Bay|    1|\n",
      "|Governor's Island...|    2|\n",
      "| Green-Wood Cemetery|    5|\n",
      "|       Broad Channel|    8|\n",
      "|     Highbridge Park|   14|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Show the 5 least frequent pickup locations\n",
    "df_zones.orderBy('count').limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144ccc39-ea68-4ad9-8bf8-254b9be5a687",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f5e09eb-200b-4333-88d2-296bbb2cb893",
   "metadata": {},
   "source": [
    "## Submitting the solutions\n",
    "\n",
    "- Form for submitting: https://courses.datatalks.club/de-zoomcamp-2024/homework/hw5\n",
    "- Deadline: See the website"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
