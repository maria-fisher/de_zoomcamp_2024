{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcc212c3-84f6-4cdc-932b-7c1352a6c89b",
   "metadata": {},
   "source": [
    "# Streaming with PySpark - RedPandas - Kafka \n",
    "### Homework 6\n",
    "#### Maria Fisher "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd810777-0362-4081-9b26-8704b8f17581",
   "metadata": {},
   "source": [
    "# Test Kafka + RedPandas \n",
    "Start Red Panda\n",
    "\n",
    "Let's start redpanda in a docker container.\n",
    "\n",
    "There's a docker-compose.yml file in the homework folder (taken from here)\n",
    "\n",
    "Copy this file to your homework directory and run\n",
    "\n",
    "docker-compose up\n",
    "\n",
    "(Add -d if you want to run in detached mode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dee4764-661b-446f-b486-a56ab30bca8b",
   "metadata": {},
   "source": [
    "# Question 1: Redpanda version\n",
    "\n",
    "Now let's find out the version of redpandas.\n",
    "\n",
    "For that, check the output of the command rpk help inside the container. The name of the container is redpanda-1.\n",
    "\n",
    "Find out what you need to execute based on the help output.\n",
    "\n",
    "What's the version, based on the output of the command you executed? (copy the entire version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029f9ba4-962d-4cf8-a6bd-d11c24628b7f",
   "metadata": {},
   "source": [
    "# Question 2. Creating a topic\n",
    "\n",
    "Before we can send data to the redpanda server, we need to create a topic. We do it also with the rpk command we used previously for figuring out the version of redpandas.\n",
    "\n",
    "Read the output of help and based on it, create a topic with name test-topic\n",
    "\n",
    "What's the output of the command for creating a topic?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8231ad-76b2-4b8b-82f5-6b34ff1d1b07",
   "metadata": {},
   "source": [
    "# Question 3. Connecting to the Kafka server\n",
    "\n",
    "We need to make sure we can connect to the server, so later we can send some data to its topics\n",
    "\n",
    "First, let's install the kafka connector (up to you if you want to have a separate virtual environment for that)\n",
    "\n",
    "pip install kafka-python\n",
    "\n",
    "You can start a jupyter notebook in your solution folder or create a script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1507c6-b8cc-4593-aba1-ee8cf9bafe92",
   "metadata": {},
   "source": [
    "### Let's try to connect to our server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c40a9e2-51ca-4a02-a9db-047884e8c7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time \n",
    "from kafka import KafkaProducer\n",
    "\n",
    "def json_serializer(data):\n",
    "    return json.dumps(data).encode('utf-8')\n",
    "\n",
    "server = 'localhost:9092'\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=[server],\n",
    "    value_serializer=json_serializer\n",
    ")\n",
    "\n",
    "try:\n",
    "    topic = 'test-topic'\n",
    "    message = {'key': 'value'}  # Your message here\n",
    "    producer.send(topic, value=message)\n",
    "    print(\"Message sent successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to send message: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7976f53e-ea83-40e0-9829-b80e28ab9277",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ae71958-54ef-4a79-bf3a-c8dad99ae7b8",
   "metadata": {},
   "source": [
    "# Question 4. Sending data to the stream\n",
    "\n",
    "## Now we're ready to send some test data:\n",
    "\n",
    "How much time did it take? Where did it spend most of the time?\n",
    "\n",
    "    Sending the messages\n",
    "    Flushing\n",
    "    Both took approximately the same amount of time\n",
    "\n",
    "(Don't remove time.sleep when answering this question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c95ce9-2ac4-41bf-ac31-2218f709e8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from kafka import KafkaProducer\n",
    "import json\n",
    "\n",
    "# Initialize KafkaProducer\n",
    "server = 'localhost:9092'\n",
    "producer = KafkaProducer(bootstrap_servers=[server], value_serializer=lambda x: json.dumps(x).encode('utf-8'))\n",
    "\n",
    "# Record the starting time for sending messages\n",
    "start_sending_time = time.time()\n",
    "\n",
    "# Define the Kafka topic\n",
    "topic_name = 'test-topic'\n",
    "\n",
    "# Send 10 messages with a delay of 0.05 seconds between each message\n",
    "for i in range(10):\n",
    "    message = {'number': i}\n",
    "    producer.send(topic_name, value=message)\n",
    "    print(f\"Sent: {message}\")\n",
    "    time.sleep(0.05)\n",
    "\n",
    "# Record the ending time for sending messages\n",
    "end_sending_time = time.time()\n",
    "\n",
    "# Record the starting time for flushing\n",
    "start_flushing_time = time.time()\n",
    "\n",
    "# Flush the producer to ensure all messages are sent\n",
    "producer.flush()\n",
    "\n",
    "# Record the ending time for flushing\n",
    "end_flushing_time = time.time()\n",
    "\n",
    "# Calculate the time taken for sending messages and flushing\n",
    "sending_time = end_sending_time - start_sending_time\n",
    "flushing_time = end_flushing_time - start_flushing_time\n",
    "\n",
    "# Print the time taken for each operation\n",
    "print(f'Time taken for sending messages: {sending_time:.2f} seconds')\n",
    "print(f'Time taken for flushing: {flushing_time:.2f} seconds')\n",
    "\n",
    "# Close the producer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf694d8-085e-44aa-ac5c-4256e7411e2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d17d0bb0-e4c8-48c3-b1f9-fba087064313",
   "metadata": {},
   "source": [
    "# Reading data with rpk\n",
    "\n",
    "You can see the messages that you send to the topic with rpk:\n",
    "\n",
    "rpk topic consume test-topic\n",
    "\n",
    "Run the command above and send the messages one more time to see them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bae5b7-1fd9-4139-bca8-c6a411a5a2d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f3d829f-51c5-4087-82c5-31c4b9000178",
   "metadata": {},
   "source": [
    "# Sending the taxi data\n",
    "\n",
    "Now let's send our actual data:\n",
    "\n",
    "    Read the green csv.gz file\n",
    "    We will only need these columns:\n",
    "        'lpep_pickup_datetime',\n",
    "        'lpep_dropoff_datetime',\n",
    "        'PULocationID',\n",
    "        'DOLocationID',\n",
    "        'passenger_count',\n",
    "        'trip_distance',\n",
    "        'tip_amount'\n",
    "\n",
    "Iterate over the records in the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f7390d0-78e3-4980-8f9a-f852e45b023c",
   "metadata": {},
   "source": [
    "# Question 5: Sending the Trip Data\n",
    "\n",
    "    Create a topic green-trips and send the data there\n",
    "    How much time in seconds did it take? (You can round it to a whole number)\n",
    "    Make sure you don't include sleeps in your code\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b4294e-a5eb-46c1-9a2d-1c9b77d05aa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79bb8ee5-976a-4f0d-ac8e-a7775fdc4335",
   "metadata": {},
   "source": [
    "# NYC green-taxi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41392a27-a4e9-4e6f-ae1f-cda28fc28b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the path to the CSV file\n",
    "data_path = 'resources/data/green_tripdata_2019-10.csv'\n",
    "\n",
    "# Define the columns to be extracted\n",
    "columns_to_extract = [\n",
    "    'lpep_pickup_datetime',\n",
    "    'lpep_dropoff_datetime',\n",
    "    'PULocationID',\n",
    "    'DOLocationID',\n",
    "    'passenger_count',\n",
    "    'trip_distance',\n",
    "    'tip_amount'\n",
    "]\n",
    "\n",
    "# Read the CSV file and extract the required columns\n",
    "df_green = pd.read_csv(data_path, usecols=columns_to_extract)\n",
    "\n",
    "# Display the first few rows of the dataframe\n",
    "print(df_green.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95bddcd-b7a6-41f9-92cb-8aa6720c52a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import Producer, KafkaException\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Initialize Kafka producer\n",
    "producer = Producer({'bootstrap.servers': 'localhost:9092', 'queue.buffering.max.ms': 1})\n",
    "\n",
    "# Define the topic\n",
    "topic = 'taxi-green'\n",
    "\n",
    "# Define the columns to be included\n",
    "columns = [\n",
    "    'lpep_pickup_datetime',\n",
    "    'lpep_dropoff_datetime',\n",
    "    'PULocationID',\n",
    "    'DOLocationID',\n",
    "    'passenger_count',\n",
    "    'trip_distance',\n",
    "    'tip_amount'\n",
    "]\n",
    "\n",
    "# Start time to measure execution time\n",
    "start_time = time.time()\n",
    "\n",
    "# Asynchronously produce messages to Kafka topic\n",
    "try:\n",
    "    for row in df_green[columns].itertuples(index=False):\n",
    "        row_dict = {col: getattr(row, col) for col in columns}\n",
    "        # Convert to JSON string\n",
    "        message = pd.Series(row_dict).to_json()\n",
    "        # Produce message to Kafka topic asynchronously\n",
    "        producer.produce(topic, message.encode('utf-8'), callback=lambda err, _: None)\n",
    "\n",
    "except BufferError as e:\n",
    "    print(f\"BufferError: {e}\")\n",
    "\n",
    "# Wait for all messages to be delivered\n",
    "producer.flush()\n",
    "\n",
    "# Calculate execution time\n",
    "execution_time = round(time.time() - start_time)\n",
    "\n",
    "print(\"Execution time (seconds):\", execution_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9ff1de-d5b8-48bb-ab1c-11a1d45409a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc71ffe7-049b-4733-a5cc-388fb221fca0",
   "metadata": {},
   "source": [
    "# Creating the PySpark consumer\n",
    "\n",
    "Now let's read the data with PySpark.\n",
    "\n",
    "Spark needs a library (jar) to be able to connect to Kafka, so we need to tell PySpark that it needs to use it:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "04d219af-8763-45fc-b8f9-57f308749e9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/14 18:57:39 WARN Utils: Your hostname, river resolves to a loopback address: 127.0.1.1; using 192.168.1.252 instead (on interface wlp1s0)\n",
      "24/03/14 18:57:39 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/malu/spark/spark-3.3.2-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/malu/.ivy2/cache\n",
      "The jars for the packages stored in: /home/malu/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-83052cdf-b00f-42d9-8a3d-5adfef1df3e4;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 in central\n",
      "\tfound org.apache.kafka#kafka-clients;3.4.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.3 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.7 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.4 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 805ms :: artifacts dl 25ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.4 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.4 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;3.4.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.5.1 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.7 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.3 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   11  |   0   |   0   |   0   ||   11  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-83052cdf-b00f-42d9-8a3d-5adfef1df3e4\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 11 already retrieved (0kB/15ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/14 18:57:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "pyspark_version = pyspark.__version__\n",
    "kafka_jar_package = f\"org.apache.spark:spark-sql-kafka-0-10_2.12:{pyspark_version}\"\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"GreenTripsConsumer\") \\\n",
    "    .config(\"spark.jars.packages\", kafka_jar_package) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3c0801-758d-4d26-a4a7-39766e5f3b01",
   "metadata": {},
   "source": [
    "### Now we can connect to the stream:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e06637e4-d727-46c9-bf61-10b3c4a03735",
   "metadata": {},
   "outputs": [],
   "source": [
    "green_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"taxi-green\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e225edb-3292-434a-bb95-3201f48a162a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae9b8436-14bd-46d1-a815-913494e72a87",
   "metadata": {},
   "source": [
    "### In order to test that we can consume from the stream, let's see what will be the first record there.\n",
    "\n",
    "In Spark streaming, the stream is represented as a sequence of small batches, each batch being a small RDD (or a small dataframe).\n",
    "\n",
    "So we can execute a function over each mini-batch. Let's run take(1) there to see what do we have in the stream:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "785a33be-fa71-4940-ac50-3671d0f95c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/14 18:57:59 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-90827a4d-3ad0-4cd8-9797-180d44919345. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/03/14 18:57:59 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(key=None, value=bytearray(b'{\"lpep_pickup_datetime\": \"2019-10-01 00:26:02\", \"lpep_dropoff_datetime\": \"2019-10-01 00:39:58\", \"PULocationID\": 112, \"DOLocationID\": 196, \"passenger_count\": 1.0, \"trip_distance\": 5.88, \"tip_amount\": 0.0}'), topic='taxi-green', partition=0, offset=0, timestamp=datetime.datetime(2024, 3, 14, 18, 9, 13, 790000), timestampType=0)\n"
     ]
    }
   ],
   "source": [
    "def peek(mini_batch, batch_id):\n",
    "    first_row = mini_batch.take(1)\n",
    "\n",
    "    if first_row:\n",
    "        print(first_row[0])\n",
    "\n",
    "query = green_stream.writeStream.foreachBatch(peek).start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b436539-afd1-45ce-b474-617815485885",
   "metadata": {},
   "source": [
    "### Now let's stop the query, so it doesn't keep consuming messages from the stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c39a834b-633a-4fd4-8107-33a90fd2ec88",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675967cc-c991-44b3-8e64-e02d58ddd3dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4af5f47-0fd7-4f92-97f8-bac9071e96d9",
   "metadata": {},
   "source": [
    "# Question 6. Parsing the data\n",
    "\n",
    "The data is JSON, but currently it's in binary format. We need to parse it and turn it into a streaming dataframe with proper columns\n",
    "\n",
    "Similarly to PySpark, we define the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b554578-8fae-4329-b390-cfed2a166e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "\n",
    "schema = types.StructType() \\\n",
    "    .add(\"lpep_pickup_datetime\", types.StringType()) \\\n",
    "    .add(\"lpep_dropoff_datetime\", types.StringType()) \\\n",
    "    .add(\"PULocationID\", types.IntegerType()) \\\n",
    "    .add(\"DOLocationID\", types.IntegerType()) \\\n",
    "    .add(\"passenger_count\", types.DoubleType()) \\\n",
    "    .add(\"trip_distance\", types.DoubleType()) \\\n",
    "    .add(\"tip_amount\", types.DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ccab172-2cd3-446b-adb2-5a15286caa0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "green_stream = green_stream \\\n",
    "  .select(F.from_json(F.col(\"value\").cast('STRING'), schema).alias(\"data\")) \\\n",
    "  .select(\"data.*\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefd740f-8b29-499d-8407-dd527291344c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7cd7f299-8dd5-4088-91eb-c927b2b170bb",
   "metadata": {},
   "source": [
    "# Question 7: Most popular destination\n",
    "\n",
    "Now let's finally do some streaming analytics. We will see what's the most popular destination currently based on our stream of data (which ideally we should have sent with delays like we did in workshop 2)\n",
    "\n",
    "This is how you can do it:\n",
    "\n",
    "    Add a column \"timestamp\" using the current_timestamp function\n",
    "    Group by:\n",
    "        5 minutes window based on the timestamp column (F.window(col(\"timestamp\"), \"5 minutes\"))\n",
    "        \"DOLocationID\"\n",
    "    Order by count\n",
    "\n",
    "You can print the output to the console using this code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c240c236-2e5f-4368-94c9-2dca55985471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/03/14 18:58:16 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "24/03/14 18:58:17 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-12564910-db08-4714-8499-a7c502017879. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/03/14 18:58:17 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+------------------------------------------+------------+-----+\n",
      "|window                                    |DOLocationID|count|\n",
      "+------------------------------------------+------------+-----+\n",
      "|{2024-03-14 18:55:00, 2024-03-14 19:00:00}|74          |43810|\n",
      "|{2024-03-14 18:55:00, 2024-03-14 19:00:00}|42          |38778|\n",
      "|{2024-03-14 18:55:00, 2024-03-14 19:00:00}|41          |35382|\n",
      "|{2024-03-14 18:55:00, 2024-03-14 19:00:00}|75          |31337|\n",
      "|{2024-03-14 18:55:00, 2024-03-14 19:00:00}|129         |28626|\n",
      "|{2024-03-14 18:55:00, 2024-03-14 19:00:00}|166         |28281|\n",
      "|{2024-03-14 18:55:00, 2024-03-14 19:00:00}|7           |28236|\n",
      "|{2024-03-14 18:55:00, 2024-03-14 19:00:00}|236         |19434|\n",
      "|{2024-03-14 18:55:00, 2024-03-14 19:00:00}|238         |18686|\n",
      "|{2024-03-14 18:55:00, 2024-03-14 19:00:00}|223         |18676|\n",
      "|{2024-03-14 18:55:00, 2024-03-14 19:00:00}|82          |16711|\n",
      "|{2024-03-14 18:55:00, 2024-03-14 19:00:00}|181         |16386|\n",
      "|{2024-03-14 18:55:00, 2024-03-14 19:00:00}|95          |16072|\n",
      "|{2024-03-14 18:55:00, 2024-03-14 19:00:00}|244         |15742|\n",
      "|{2024-03-14 18:55:00, 2024-03-14 19:00:00}|138         |15421|\n",
      "|{2024-03-14 18:55:00, 2024-03-14 19:00:00}|116         |15220|\n",
      "|{2024-03-14 18:55:00, 2024-03-14 19:00:00}|97          |13555|\n",
      "|{2024-03-14 18:55:00, 2024-03-14 19:00:00}|151         |12612|\n",
      "|{2024-03-14 18:55:00, 2024-03-14 19:00:00}|61          |12522|\n",
      "|{2024-03-14 18:55:00, 2024-03-14 19:00:00}|49          |11792|\n",
      "+------------------------------------------+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import window, current_timestamp\n",
    "\n",
    "# Create SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Most Popular Destination\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Assuming green_stream is the parsed DataFrame from Question 6\n",
    "\n",
    "# Add a column \"timestamp\" using the current_timestamp function\n",
    "green_stream_with_timestamp = green_stream.withColumn(\"timestamp\", current_timestamp())\n",
    "\n",
    "# Group by window and DOLocationID, and order by count\n",
    "popular_destinations = green_stream_with_timestamp \\\n",
    "    .groupBy(window(\"timestamp\", \"5 minutes\"), \"DOLocationID\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"count\", ascending=False)\n",
    "\n",
    "# Write to console\n",
    "query = popular_destinations \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
